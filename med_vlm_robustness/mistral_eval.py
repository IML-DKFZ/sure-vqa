from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import json
import torch
import ndjson
from tqdm import tqdm
from pathlib import Path
import re

from utils import get_config

MISTRAL_PROMPT_FILE_PATH = os.getenv("MISTRAL_PROMPT_FILE_PATH")

def mistal_eval(config):
    mistral_output_list=[]
    model_output_file = config.model_output_file
    mistral_eval_file = config.metrics_file

    # Load model and tokenizer from huggingface
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
    # load the model with float16 to fit the memory
    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", device_map='cuda', torch_dtype=torch.bfloat16)

    # read the initial prompt
    with open(MISTRAL_PROMPT_FILE_PATH,'r') as f:
        initial_prompt = f.read()
    
    # Load the output JSON data generated by the model
    with open(model_output_file, 'r') as file:
        model_output_data = json.load(file)

    # Iterate over each item in the JSON array
    for line in tqdm(model_output_data):
        qid = line["qid"]
        question = line["question"]
        gt = line["gt"]
        pred = line["pred"]
        answer_type = line["answer_type"]
        
        complete_input = initial_prompt + "[INST] " + str(line) + " [/INST]"
        input_ids = tokenizer(complete_input, return_tensors="pt").input_ids.to("cuda")

        with torch.inference_mode():
            output_ids = model.generate(
                    input_ids,
                    max_new_tokens=200,
                )
        input_token_len = input_ids.shape[1]
        outputs = tokenizer.batch_decode(
            output_ids[:, input_token_len:], skip_special_tokens=True
        )[0]
        outputs = outputs.strip()

        # find the score value in th emodel output
        score_search_obj=re.search("###mistralscore=(.*)###",outputs)
        mistral_score=score_search_obj.group(1)

        # TODO: add try catch
        # create a dict from including the mdoel score
        output_dict={
            "qid": qid,
            "question": question,
            "gt": gt,
            "pred": pred,
            "answer_type": answer_type,
            "mistral_score": int(mistral_score)
        }
        mistral_output_list.append(output_dict)
        break

    # save mistral evaluation as JSON
    if not Path(mistral_eval_file).parent.is_dir():
        os.makedirs(Path(mistral_eval_file).parent)
    with open("deneme", 'w') as json_file:
        json.dump(mistral_output_list, json_file, indent=4)

if __name__ == '__main__':
    config = get_config()
    mistal_eval(config)