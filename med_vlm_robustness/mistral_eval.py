from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import json
import torch
import ndjson
from tqdm import tqdm
from pathlib import Path
import re

from utils import get_config

MISTRAL_PROMPT_FILE_PATH = os.getenv("MISTRAL_PROMPT_FILE_PATH")

def mistal_eval(config):
    mistral_output_list=[]
    model_output_file = config.model_output_file
    mistral_eval_file = config.metrics_file

    # Load model and tokenizer from huggingface
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
    # load the model with float16 to fit the memory
    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", device_map='cuda', torch_dtype=torch.bfloat16)

    # read the initial prompt
    with open(MISTRAL_PROMPT_FILE_PATH,'r') as f:
        initial_prompt = f.read()
    
    # Load the output JSON data generated by the model
    with open(model_output_file, 'r') as file:
        model_output_data = json.load(file)

    # Iterate over each item in the JSON array
    for line in tqdm(model_output_data):
        qid = line["qid"]
        question = line["question"]
        gt = line["gt"]
        pred = line["pred"]
        answer_type = line["answer_type"]
        
        complete_input = initial_prompt + "[INST] " + str(line) + " [/INST]"
        input_ids = tokenizer(complete_input, return_tensors="pt").input_ids.to("cuda")

        with torch.inference_mode():
            output_ids = model.generate(
                    input_ids,
                    max_new_tokens=200,
                )
        input_token_len = input_ids.shape[1]
        outputs = tokenizer.batch_decode(
            output_ids[:, input_token_len:], skip_special_tokens=True
        )[0]
        outputs = outputs.strip()

        # find the score value in the model output
        try:
            score_obj = re.search("###mistralscore=(.*)###", outputs)
            if score_obj:
                mistral_score = float(score_obj.group(1))
            else:
                # Handle the case where the pattern is not found in the outputs
                # make the mistral score store the complete output so that it can be analyzed later
                mistral_score = outputs
                print("Mistral score not found.")
                print("The Mistral score for this instance will be assigned to the complete output of the Mistral model.")
                print("Inspect this in the output JSON file")
        except AttributeError:
            # Handle other potential attribute errors, e.g., group not found
            print("Error while finding the mistral score")
        except TypeError:
            # handle the case where the mistral score is not numeric therefore it cannot be converted to int() 
            print("Retrieved mistral score object is being covnerted to float but it is not an integer")

        # create a dict from including the mdoel score
        output_dict={
            "qid": qid,
            "question": question,
            "gt": gt,
            "pred": pred,
            "answer_type": answer_type,
            "mistral_score": mistral_score
        }
        mistral_output_list.append(output_dict)

    # save mistral evaluation as JSON
    if not Path(mistral_eval_file).parent.is_dir():
        os.makedirs(Path(mistral_eval_file).parent)
    with open(mistral_eval_file, 'w') as json_file:
        json.dump(mistral_output_list, json_file, indent=4)

if __name__ == '__main__':
    config = get_config()
    mistal_eval(config)