from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import json
import torch
import ndjson
from tqdm import tqdm
from pathlib import Path

from utils import get_config

MISTRAL_PROMPT_FILE_PATH = os.getenv("MISTRAL_PROMPT_FILE_PATH")

def mistal_eval(config):

    model_output_file = config.model_output_file
    mistral_eval_file = config.metrics_file

    # Load model and tokenizer from huggingface
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
    # load the model with float16 to fit the memory
    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", device_map='cuda', torch_dtype=torch.bfloat16)

    # read the initial prompt
    with open(MISTRAL_PROMPT_FILE_PATH,'r') as f:
        initial_prompt = f.read()
    
    # Load the output JSON data generated by the model
    with open(model_output_file, 'r') as file:
        json_file = file.read()
    model_output_data = json.loads(json_file)

    mistral_output_list=[]
    # Iterate over each item in the JSON array
    for line in tqdm(model_output_data):
        complete_input = initial_prompt + "[INST] " + str(line) + " [/INST]"
        input_ids = tokenizer(complete_input, return_tensors="pt").input_ids.to("cuda")
        output_ids = model.generate(
                input_ids,
                max_new_tokens=200,
            )
        input_token_len = input_ids.shape[1]
        outputs = tokenizer.batch_decode(
            output_ids[:, input_token_len:], skip_special_tokens=True
        )[0]
        outputs = outputs.strip()
        mistral_output_list.append(json.dumps(outputs))

    # save mistral evaluation as JSON
    if not Path(mistral_eval_file).parent.is_dir():
        os.makedirs(Path(mistral_eval_file).parent)
    with open(mistral_eval_file, 'w') as json_file:
        json.dump(mistral_output_list,json_file, indent=2)

if __name__ == '__main__':
    config = get_config()
    mistal_eval(config)